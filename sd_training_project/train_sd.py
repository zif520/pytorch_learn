#!/usr/bin/env python3
"""
Stable Diffusion 1.5 è®­ç»ƒè„šæœ¬ - åˆå­¦è€…ç‰ˆæœ¬
é€‚ç”¨äº macOS ç³»ç»Ÿ
"""

import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import torch
import numpy as np
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from diffusers import (
    StableDiffusionPipeline,
    UNet2DConditionModel,
    DDPMScheduler,
    AutoencoderKL
)
from transformers import CLIPTokenizer, CLIPTextModel
from tqdm import tqdm
import argparse
from pathlib import Path
import torchvision
from torchvision import datasets, transforms
from datasets import load_dataset

class SimpleDataset(Dataset):
    """ç®€å•çš„æ•°æ®é›†ç±»ï¼Œç”¨äºåŠ è½½å›¾ç‰‡å’Œæ–‡æœ¬æè¿°"""
    
    def __init__(self, data_dir, tokenizer, image_size=512):
        self.data_dir = Path(data_dir)
        self.tokenizer = tokenizer
        self.image_size = image_size
        
        # è¯»å– captions.txt æ–‡ä»¶
        captions_file = self.data_dir / "captions.txt"
        if not captions_file.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° captions.txt æ–‡ä»¶: {captions_file}")
        
        self.samples = []
        with open(captions_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and '\t' in line:
                    img_name, caption = line.split('\t', 1)
                    img_path = self.data_dir / img_name
                    if img_path.exists():
                        self.samples.append((img_path, caption))
        
        print(f"åŠ è½½äº† {len(self.samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, caption = self.samples[idx]
        
        # åŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡
        image = Image.open(img_path).convert("RGB")
        image = image.resize((self.image_size, self.image_size))
        image = np.array(image).astype(np.float32) / 255.0
        image = torch.from_numpy(image).permute(2, 0, 1)  # HWC -> CHW
        
        # ç¼–ç æ–‡æœ¬
        tokenized = self.tokenizer(
            caption,
            padding="max_length",
            truncation=True,
            max_length=77,
            return_tensors="pt"
        )
        
        return {
            "pixel_values": image,
            "input_ids": tokenized.input_ids[0]
        }

class HFCIFAR10Dataset(Dataset):
    def __init__(self, split='train', image_size=32):
        self.dataset = load_dataset('cifar10', split=split)
        self.image_size = image_size
        self.label_names = self.dataset.features['label'].names

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        image = item['img'].resize((self.image_size, self.image_size))
        image = np.array(image).astype(np.float32) / 255.0
        image = torch.from_numpy(image).permute(2, 0, 1)
        caption = self.label_names[item['label']]
        return image, caption

class CIFAR10SDWrapper(Dataset):
    def __init__(self, cifar_dataset, tokenizer):
        self.cifar_dataset = cifar_dataset
        self.tokenizer = tokenizer
    def __len__(self):
        return len(self.cifar_dataset)
    def __getitem__(self, idx):
        image, caption = self.cifar_dataset[idx]
        tokenized = self.tokenizer(
            caption,
            padding="max_length",
            truncation=True,
            max_length=77,
            return_tensors="pt"
        )
        return {
            "pixel_values": image,
            "input_ids": tokenized.input_ids[0]
        }

def load_models(model_id="runwayml/stable-diffusion-v1-5"):
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_id}")
    
    # åŠ è½½ tokenizer å’Œ text encoder
    tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder="text_encoder")
    
    # åŠ è½½ UNet
    unet = UNet2DConditionModel.from_pretrained(model_id, subfolder="unet")
    
    # åŠ è½½ VAE
    vae = AutoencoderKL.from_pretrained(model_id, subfolder="vae")
    
    # åŠ è½½ scheduler
    scheduler = DDPMScheduler.from_pretrained(model_id, subfolder="scheduler")
    
    return tokenizer, text_encoder, unet, vae, scheduler

def train_step(batch, unet, text_encoder, vae, scheduler, optimizer, device):
    """å•æ­¥è®­ç»ƒ"""
    # å°†æ•°æ®ç§»åˆ°è®¾å¤‡ä¸Š
    pixel_values = batch["pixel_values"].to(device)
    input_ids = batch["input_ids"].to(device)
    
    # å†»ç»“ text encoder å’Œ vae
    with torch.no_grad():
        # ç¼–ç æ–‡æœ¬
        text_embeddings = text_encoder(input_ids)[0]
        
        # ç¼–ç å›¾ç‰‡
        latents = vae.encode(pixel_values).latent_dist.sample()
        latents = latents * 0.18215  # ç¼©æ”¾å› å­
    
    # æ·»åŠ å™ªå£°
    noise = torch.randn_like(latents)
    bsz = latents.shape[0]
    timesteps = torch.randint(0, scheduler.num_train_timesteps, (bsz,), device=latents.device)
    noisy_latents = scheduler.add_noise(latents, noise, timesteps)
    
    # é¢„æµ‹å™ªå£°
    noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=text_embeddings).sample
    
    # è®¡ç®—æŸå¤±
    loss = torch.nn.functional.mse_loss(noise_pred, noise, reduction="mean")
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    return loss.item()

def main():
    parser = argparse.ArgumentParser(description="Stable Diffusion 1.5 è®­ç»ƒè„šæœ¬")
    parser.add_argument("--data_dir", type=str, default=None, help="æ•°æ®ç›®å½•è·¯å¾„ï¼ˆå¦‚ä¸æŒ‡å®šåˆ™ç”¨CIFAR-10ï¼‰")
    parser.add_argument("--output_dir", type=str, default="output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--epochs", type=int, default=10, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=1, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=1e-5, help="å­¦ä¹ ç‡")
    parser.add_argument("--save_steps", type=int, default=100, help="ä¿å­˜æ­¥æ•°")
    parser.add_argument("--image_size", type=int, default=32, help="å›¾ç‰‡å°ºå¯¸ï¼ˆCIFAR-10ä¸º32ï¼‰")
    args = parser.parse_args()
    
    # æ£€æŸ¥è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ä½¿ç”¨ MPS (Apple Silicon)")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print("ä½¿ç”¨ CUDA")
    else:
        device = torch.device("cpu")
        print("ä½¿ç”¨ CPU")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # åŠ è½½æ¨¡å‹
    tokenizer, text_encoder, unet, vae, scheduler = load_models()
    
    # å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡ä¸Š
    text_encoder = text_encoder.to(device)
    unet = unet.to(device)
    vae = vae.to(device)
    
    # å†»ç»“ text encoder å’Œ vae
    text_encoder.requires_grad_(False)
    vae.requires_grad_(False)
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    if args.data_dir is None:
        print("ä½¿ç”¨Hugging Face Hubä¸Šçš„CIFAR-10æ•°æ®é›†")
        dataset = HFCIFAR10Dataset(split='train', image_size=args.image_size)
        dataset = CIFAR10SDWrapper(dataset, tokenizer)
    else:
        print(f"ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†: {args.data_dir}")
        dataset = SimpleDataset(args.data_dir, tokenizer, image_size=args.image_size)
    # ä¼˜åŒ–ï¼šåŠ é€Ÿæ•°æ®åŠ è½½
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)
    
    # è®¾å¤‡æç¤º
    if device.type == "mps":
        print("\nğŸ’¡ å»ºè®®ï¼šä½ æ­£åœ¨ä½¿ç”¨Apple Silicon MPSåŠ é€Ÿï¼Œå¯ä»¥å°è¯•é€‚å½“è°ƒå¤§ batch_sizeï¼ˆå¦‚2ã€4ï¼‰ï¼Œä»¥æå‡è®­ç»ƒé€Ÿåº¦ã€‚\n")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(unet.parameters(), lr=args.learning_rate)
    
    # è®­ç»ƒå¾ªç¯
    print(f"å¼€å§‹è®­ç»ƒï¼Œå…± {args.epochs} è½®")
    global_step = 0
    
    for epoch in range(args.epochs):
        print(f"\nç¬¬ {epoch + 1}/{args.epochs} è½®è®­ç»ƒ")
        
        epoch_losses = []
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch + 1}")
        
        for batch in progress_bar:
            loss = train_step(batch, unet, text_encoder, vae, scheduler, optimizer, device)
            epoch_losses.append(loss)
            global_step += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({"loss": f"{loss:.4f}"})
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if global_step % args.save_steps == 0:
                save_path = output_dir / f"unet_step_{global_step}"
                unet.save_pretrained(save_path)
                print(f"\næ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
        
        # æ¯è½®ç»“æŸåä¿å­˜
        avg_loss = np.mean(epoch_losses)
        print(f"ç¬¬ {epoch + 1} è½®å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        save_path = output_dir / f"unet_epoch_{epoch + 1}"
        unet.save_pretrained(save_path)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_save_path = output_dir / "unet_final"
    unet.save_pretrained(final_save_path)
    print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_save_path}")

if __name__ == "__main__":
    main() 